{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install fairseq npy_append_array h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 20:36:24 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fairseq\n",
    "import h5py\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from npy_append_array import NpyAppendArray\n",
    "from torch.hub import download_url_to_file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive Bark semantic token codebook\n",
    "\n",
    "This notebook embeds a synthetic dataset of wav files <-> semantic token mappings and rederives the \"means\" for each token, allowing for later routine k-means inference of \"ground truth\" audio into semantic prompts. This allows for voice cloning and so giving voice to the dead, destroying consensus reality and creating misinformation, destroying copyright, spreading systemic bias, and other similarly enjoyable ways to spend a Saturday afternoon."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump HuBERT features\n",
    "\n",
    "In this step, we use Fairseq's [Sharded HuBERT feature extraction](https://github.com/facebookresearch/fairseq/tree/main/examples/hubert/simple_kmeans) to obtain embeddings of the audio corresponding to each semantic token step (HuBERT features are 50hz by default, coincidence???)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../models/hubert_base_ls960.pt\"):\n",
    "    # Yes, hard-coding the URL of the model is jank. Too bad!\n",
    "    # Update this if this changes! https://github.com/facebookresearch/textlesslib/blob/698e6a039375bac0cd5f1b8683beeec5e8f702c0/textless/checkpoint_manager/__init__.py#L20\n",
    "    download_url_to_file(\"https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt\", \"../models/hubert_base_ls960.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the git repo root directory\n",
    "git_root = subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"]).strip().decode(\"utf-8\")\n",
    "\n",
    "# Append the desired subdirectory\n",
    "feature_utils_path = os.path.join(git_root, \"venv\", \"lib\", \"python3.10\", \"site-packages\", \"fairseq\", \"examples\", \"hubert\", \"simple_kmeans\")\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(feature_utils_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(\"..\", \"datasets\", \"en\")\n",
    "feature_dir = os.path.join(\"..\", \"datasets\", \"en_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 20:37:45 | INFO | fairseq.tasks.hubert_pretraining | current directory is /home/ritsuko/projects/ai/audio/bark/notebooks\n",
      "2023-04-27 20:37:45 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2023-04-27 20:37:45 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
      "2023-04-27 20:37:46 | INFO | dump_hubert_feature | TASK CONFIG:\n",
      "{'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2023-04-27 20:37:46 | INFO | dump_hubert_feature |  max_chunk = 1600000\n",
      "2023-04-27 20:37:46 | INFO | feature_utils | rank 0 of 8, process 1628 (0-1628) out of 13020\n",
      "100%|██████████| 1628/1628 [00:12<00:00, 127.05it/s]\n",
      "2023-04-27 20:37:58 | INFO | feature_utils | finished successfully\n",
      "2023-04-27 20:38:00 | INFO | fairseq.tasks.hubert_pretraining | current directory is /home/ritsuko/projects/ai/audio/bark/notebooks\n",
      "2023-04-27 20:38:00 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2023-04-27 20:38:00 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
      "2023-04-27 20:38:01 | INFO | dump_hubert_feature | TASK CONFIG:\n",
      "{'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2023-04-27 20:38:01 | INFO | dump_hubert_feature |  max_chunk = 1600000\n",
      "2023-04-27 20:38:01 | INFO | feature_utils | rank 1 of 8, process 1627 (1628-3255) out of 13020\n",
      "100%|██████████| 1627/1627 [00:08<00:00, 196.43it/s]\n",
      "2023-04-27 20:38:09 | INFO | feature_utils | finished successfully\n",
      "2023-04-27 20:38:11 | INFO | fairseq.tasks.hubert_pretraining | current directory is /home/ritsuko/projects/ai/audio/bark/notebooks\n",
      "2023-04-27 20:38:11 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2023-04-27 20:38:11 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
      "2023-04-27 20:38:12 | INFO | dump_hubert_feature | TASK CONFIG:\n",
      "{'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2023-04-27 20:38:12 | INFO | dump_hubert_feature |  max_chunk = 1600000\n",
      "2023-04-27 20:38:12 | INFO | feature_utils | rank 2 of 8, process 1627 (3255-4882) out of 13020\n",
      " 80%|███████▉  | 1297/1627 [00:06<00:01, 205.46it/s]2023-04-27 20:38:19 | WARNING | root | ref 52480 != read 40960 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4547_We just lost our heads and acted like a mob..wav)\n",
      "2023-04-27 20:38:19 | WARNING | root | ref 28800 != read 64640 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4548_I was right there and saw it with my own eyes..wav)\n",
      "2023-04-27 20:38:19 | WARNING | root | ref 49280 != read 27094 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4549_Where did he keep his money?.wav)\n",
      "2023-04-27 20:38:19 | WARNING | root | ref 63787 != read 37760 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4550_Walk in the grass in my bare feet?.wav)\n",
      "2023-04-27 20:38:19 | WARNING | root | ref 44374 != read 51200 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4551_Was it luck that I avoided the spoiled cake?.wav)\n",
      "2023-04-27 20:38:19 | WARNING | root | ref 53547 != read 70187 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4552_No one can be caught in places he does not visit..wav)\n",
      "2023-04-27 20:38:19 | WARNING | root | ref 24534 != read 30934 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4553_What could it be?.wav)\n",
      "2023-04-27 20:38:19 | WARNING | root | ref 29654 != read 41814 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4554_What was your favorite thing to see in your travels?.wav)\n",
      " 81%|████████  | 1318/1627 [00:07<00:01, 173.57it/s]2023-04-27 20:38:19 | WARNING | root | ref 69547 != read 52480 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4555_The urge to write short stories is rare..wav)\n",
      "2023-04-27 20:38:19 | WARNING | root | ref 67627 != read 150827 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4556_My sisters aren’t telling me everything I need to know to get there..wav)\n",
      "2023-04-27 20:38:19 | WARNING | root | ref 44800 != read 52694 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4557_Have you seen my USB dongle?.wav)\n",
      "2023-04-27 20:38:19 | WARNING | root | ref 85760 != read 74667 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4558_My teacher always throws chalk at his pupils, that's why I'm wearing a helmet..wav)\n",
      "2023-04-27 20:38:19 | WARNING | root | ref 34134 != read 35200 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4559_She's accused of stealing a watch..wav)\n",
      "2023-04-27 20:38:19 | WARNING | root | ref 59520 != read 32640 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4560_All of the resources were used up..wav)\n",
      "2023-04-27 20:38:19 | WARNING | root | ref 112427 != read 91947 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4561_No compliments, pray Without ceremony, Without the organ, Without repetition..wav)\n",
      "2023-04-27 20:38:19 | WARNING | root | ref 24960 != read 22614 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4562_What makes you think so?.wav)\n",
      "2023-04-27 20:38:19 | WARNING | root | ref 49920 != read 119467 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4563_\"But my heart is agitated,\" the boy said..wav)\n",
      "2023-04-27 20:38:19 | WARNING | root | ref 39680 != read 50774 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4564_And he vanished around the corner of the plaza..wav)\n",
      "2023-04-27 20:38:19 | WARNING | root | ref 68907 != read 77440 (/home/ritsuko/projects/ai/audio/bark/datasets/en/en_4565_If a person is living out his destiny, he knows everything he needs to know..wav)\n",
      "100%|██████████| 1627/1627 [00:08<00:00, 196.62it/s]\n",
      "2023-04-27 20:38:20 | INFO | feature_utils | finished successfully\n",
      "2023-04-27 20:38:22 | INFO | fairseq.tasks.hubert_pretraining | current directory is /home/ritsuko/projects/ai/audio/bark/notebooks\n",
      "2023-04-27 20:38:22 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2023-04-27 20:38:22 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
      "2023-04-27 20:38:23 | INFO | dump_hubert_feature | TASK CONFIG:\n",
      "{'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2023-04-27 20:38:23 | INFO | dump_hubert_feature |  max_chunk = 1600000\n",
      "2023-04-27 20:38:23 | INFO | feature_utils | rank 3 of 8, process 1628 (4882-6510) out of 13020\n",
      "100%|██████████| 1628/1628 [00:06<00:00, 259.55it/s]\n",
      "2023-04-27 20:38:29 | INFO | feature_utils | finished successfully\n",
      "2023-04-27 20:38:30 | INFO | fairseq.tasks.hubert_pretraining | current directory is /home/ritsuko/projects/ai/audio/bark/notebooks\n",
      "2023-04-27 20:38:30 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2023-04-27 20:38:30 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
      "2023-04-27 20:38:31 | INFO | dump_hubert_feature | TASK CONFIG:\n",
      "{'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2023-04-27 20:38:31 | INFO | dump_hubert_feature |  max_chunk = 1600000\n",
      "2023-04-27 20:38:31 | INFO | feature_utils | rank 4 of 8, process 1628 (6510-8138) out of 13020\n",
      "100%|██████████| 1628/1628 [00:06<00:00, 257.79it/s]\n",
      "2023-04-27 20:38:38 | INFO | feature_utils | finished successfully\n",
      "2023-04-27 20:38:39 | INFO | fairseq.tasks.hubert_pretraining | current directory is /home/ritsuko/projects/ai/audio/bark/notebooks\n",
      "2023-04-27 20:38:39 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2023-04-27 20:38:39 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
      "2023-04-27 20:38:40 | INFO | dump_hubert_feature | TASK CONFIG:\n",
      "{'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2023-04-27 20:38:40 | INFO | dump_hubert_feature |  max_chunk = 1600000\n",
      "2023-04-27 20:38:40 | INFO | feature_utils | rank 5 of 8, process 1627 (8138-9765) out of 13020\n",
      "100%|██████████| 1627/1627 [00:06<00:00, 263.03it/s]\n",
      "2023-04-27 20:38:46 | INFO | feature_utils | finished successfully\n",
      "2023-04-27 20:38:48 | INFO | fairseq.tasks.hubert_pretraining | current directory is /home/ritsuko/projects/ai/audio/bark/notebooks\n",
      "2023-04-27 20:38:48 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2023-04-27 20:38:48 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
      "2023-04-27 20:38:49 | INFO | dump_hubert_feature | TASK CONFIG:\n",
      "{'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2023-04-27 20:38:49 | INFO | dump_hubert_feature |  max_chunk = 1600000\n",
      "2023-04-27 20:38:49 | INFO | feature_utils | rank 6 of 8, process 1627 (9765-11392) out of 13020\n",
      "100%|██████████| 1627/1627 [00:09<00:00, 167.66it/s]\n",
      "2023-04-27 20:38:59 | INFO | feature_utils | finished successfully\n",
      "2023-04-27 20:39:00 | INFO | fairseq.tasks.hubert_pretraining | current directory is /home/ritsuko/projects/ai/audio/bark/notebooks\n",
      "2023-04-27 20:39:00 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2023-04-27 20:39:00 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
      "2023-04-27 20:39:01 | INFO | dump_hubert_feature | TASK CONFIG:\n",
      "{'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2023-04-27 20:39:01 | INFO | dump_hubert_feature |  max_chunk = 1600000\n",
      "2023-04-27 20:39:01 | INFO | feature_utils | rank 7 of 8, process 1628 (11392-13020) out of 13020\n",
      "100%|██████████| 1628/1628 [00:08<00:00, 201.70it/s]\n",
      "2023-04-27 20:39:09 | INFO | feature_utils | finished successfully\n"
     ]
    }
   ],
   "source": [
    "from fairseq.examples.hubert.simple_kmeans import feature_utils\n",
    "from fairseq.examples.hubert.simple_kmeans import dump_hubert_feature\n",
    "\n",
    "NSHARDS=8\n",
    "for i in range(0, NSHARDS):\n",
    "    dump_hubert_feature.main(\n",
    "        tsv_dir=dataset_dir,\n",
    "        split=\"manifest\",\n",
    "        ckpt_path=os.path.join(\"..\", \"models\", \"hubert_base_ls960.pt\"),\n",
    "        layer=6,\n",
    "        # Dataset isn't _that_ big for now; process all at once (shards: number of fractions, rank: selected fraction)\n",
    "        nshard=NSHARDS,\n",
    "        rank=i,\n",
    "        feat_dir=feature_dir,\n",
    "        max_chunk=1_600_000\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging embeddings by semantic token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44131/575906701.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  token_array = np.array(data)\n"
     ]
    }
   ],
   "source": [
    "# Yes, this is inefficient, but for n ~= 10^4 generations, it's probably not worth doing something fancier\n",
    "# Revisit this if it becomes a bottleneck!\n",
    "label_path = os.path.join(dataset_dir, \"labels.txt\")\n",
    "\n",
    "with open(label_path, 'r') as file:\n",
    "    data = []\n",
    "    for line in file:\n",
    "        # Split the line by whitespace and convert each item to an integer\n",
    "        int_list = [int(x) for x in line.split()]\n",
    "        data.append(int_list)\n",
    "\n",
    "# Convert the list of lists to a NumPy array\n",
    "token_array = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lengs = np.loadtxt(\"../datasets/en_features/manifest_0_8.len\", dtype=int)\n",
    "test_lengs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_shard_to_hdf5(\n",
    "    f,\n",
    "    feat_dir,\n",
    "    split,\n",
    "    rank, \n",
    "    shards,\n",
    "    track_idx_start,\n",
    "    tokens_by_track\n",
    "):\n",
    "    logger = logging.getLogger(\"partition_by_token\")\n",
    "\n",
    "    feat_path = f\"{feat_dir}/{split}_{rank}_{NSHARDS}.npy\"\n",
    "    leng_path = f\"{feat_dir}/{split}_{rank}_{NSHARDS}.len\"\n",
    "\n",
    "    with open(leng_path, \"r\") as outfile:\n",
    "        # List of lengths for each track on the shard\n",
    "        #lengs = [int(line.rstrip()) for line in outfile]\n",
    "        lengs = np.loadtxt(leng_path, dtype=int)\n",
    "        # Feature start indices for each track on the shard\n",
    "        offsets = np.hstack(([0], np.cumsum(lengs[:-1])))\n",
    "    \n",
    "    features = np.load(feat_path, mmap_mode=\"r\")\n",
    "    logger.info(f\"Processing {len(lengs)} tracks, {len(features)} token-embed maps\")\n",
    "\n",
    "    track_idx = track_idx_start\n",
    "    # For each track on the shard:\n",
    "    for (i, leng) in enumerate(lengs):\n",
    "        track_tokens = tokens_by_track[track_idx]\n",
    "        logger.debug(f\"Track {track_idx}: {len(track_tokens)} semantic, {leng} embeddings\")\n",
    "        # Hertz for semantic tokens and features should be same, modulo the padding token\n",
    "        if len(track_tokens) > leng + 1:\n",
    "            logger.error(f\"Track {track_idx} mismatch: {len(track_tokens)} semantic but {leng} embeddings\")\n",
    "        assert len(track_tokens) <= leng + 1\n",
    "\n",
    "        track_embeddings = features[offsets[i]:offsets[i] + leng, :]\n",
    "        logger.debug(f\"{track_embeddings.shape} retrieved\")\n",
    "        for i, (token, emb) in enumerate(zip(track_tokens, track_embeddings)):\n",
    "            # Add to hdf5 file\n",
    "            if str(token) not in f:\n",
    "                f.create_group(str(token))\n",
    "            token_group = f[str(token)]\n",
    "            emb_id = f\"{track_idx}_{i}\"\n",
    "            token_group.create_dataset(emb_id, data=emb)\n",
    "\n",
    "        track_idx += 1\n",
    "\n",
    "    # If started at track 0 and processed 8 tracks, return 8\n",
    "    return track_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_datasets(group):\n",
    "    return sum(1 for _ in group.values() if isinstance(_, h5py.Dataset))\n",
    "\n",
    "def partition_by_token(feat_dir, split, shards, tokens_by_track, rank_start=0):\n",
    "    # HORRIBLE BAD FIX THIS\n",
    "    track_idx = 1628 + 1627 + 1627\n",
    "    logger = logging.getLogger(\"partition_by_token\")\n",
    "\n",
    "    for shard_idx in range(rank_start, NSHARDS):\n",
    "        hdf5_file_path = os.path.join(feat_dir, f\"embeds_by_token_{shard_idx}_{shards}.h5\")\n",
    "        with h5py.File(hdf5_file_path, 'a') as hdf5_file:\n",
    "            new_idx = save_shard_to_hdf5(\n",
    "                f=hdf5_file,\n",
    "                feat_dir=feat_dir,\n",
    "                split=split,\n",
    "                rank=shard_idx,\n",
    "                shards=shards,\n",
    "                track_idx_start=track_idx,\n",
    "                tokens_by_track=tokens_by_track\n",
    "            )\n",
    "            # TODO: Log this instead!\n",
    "            logger.info(f\"Shard {shard_idx} processed\")\n",
    "            track_idx = new_idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 22:29:42 | INFO | partition_by_token | Processing 1628 tracks, 306554 token-embed maps\n",
      "2023-04-27 22:29:42 | ERROR | partition_by_token | Track 3255 mismatch: 256 semantic but 191 embeddings\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m logging\u001b[39m.\u001b[39mbasicConfig(\n\u001b[1;32m      2\u001b[0m     \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%(asctime)s\u001b[39;00m\u001b[39m | \u001b[39m\u001b[39m%(levelname)s\u001b[39;00m\u001b[39m | \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m | \u001b[39m\u001b[39m%(message)s\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     datefmt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     stream\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstdout,\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m partition_by_token(\n\u001b[1;32m     10\u001b[0m     feat_dir\u001b[39m=\u001b[39;49mfeature_dir,\n\u001b[1;32m     11\u001b[0m     split\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmanifest\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     rank_start\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m     shards\u001b[39m=\u001b[39;49mNSHARDS,\n\u001b[1;32m     14\u001b[0m     tokens_by_track\u001b[39m=\u001b[39;49mtoken_array\n\u001b[1;32m     15\u001b[0m )\n",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m, in \u001b[0;36mpartition_by_token\u001b[0;34m(feat_dir, split, shards, tokens_by_track, rank_start)\u001b[0m\n\u001b[1;32m      9\u001b[0m hdf5_file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(feat_dir, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39membeds_by_token_\u001b[39m\u001b[39m{\u001b[39;00mshard_idx\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mshards\u001b[39m}\u001b[39;00m\u001b[39m.h5\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39mFile(hdf5_file_path, \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m hdf5_file:\n\u001b[0;32m---> 11\u001b[0m     new_idx \u001b[39m=\u001b[39m save_shard_to_hdf5(\n\u001b[1;32m     12\u001b[0m         f\u001b[39m=\u001b[39;49mhdf5_file,\n\u001b[1;32m     13\u001b[0m         feat_dir\u001b[39m=\u001b[39;49mfeat_dir,\n\u001b[1;32m     14\u001b[0m         split\u001b[39m=\u001b[39;49msplit,\n\u001b[1;32m     15\u001b[0m         rank\u001b[39m=\u001b[39;49mshard_idx,\n\u001b[1;32m     16\u001b[0m         shards\u001b[39m=\u001b[39;49mshards,\n\u001b[1;32m     17\u001b[0m         track_idx_start\u001b[39m=\u001b[39;49mtrack_idx,\n\u001b[1;32m     18\u001b[0m         tokens_by_track\u001b[39m=\u001b[39;49mtokens_by_track\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     20\u001b[0m     \u001b[39m# TODO: Log this instead!\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShard \u001b[39m\u001b[39m{\u001b[39;00mshard_idx\u001b[39m}\u001b[39;00m\u001b[39m processed\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 33\u001b[0m, in \u001b[0;36msave_shard_to_hdf5\u001b[0;34m(f, feat_dir, split, rank, shards, track_idx_start, tokens_by_track)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(track_tokens) \u001b[39m>\u001b[39m leng \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     32\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrack \u001b[39m\u001b[39m{\u001b[39;00mtrack_idx\u001b[39m}\u001b[39;00m\u001b[39m mismatch: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(track_tokens)\u001b[39m}\u001b[39;00m\u001b[39m semantic but \u001b[39m\u001b[39m{\u001b[39;00mleng\u001b[39m}\u001b[39;00m\u001b[39m embeddings\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(track_tokens) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m leng \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     35\u001b[0m track_embeddings \u001b[39m=\u001b[39m features[offsets[i]:offsets[i] \u001b[39m+\u001b[39m leng, :]\n\u001b[1;32m     36\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtrack_embeddings\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m retrieved\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    #level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n",
    "    level=\"DEBUG\",\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "partition_by_token(\n",
    "    feat_dir=feature_dir,\n",
    "    split=\"manifest\",\n",
    "    rank_start=3,\n",
    "    shards=NSHARDS,\n",
    "    tokens_by_track=token_array\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = '../datasets/en_features/embeds_by_token.h5'\n",
    "output_filename = '../datasets/en_features/mean_embeds_by_token.h5'\n",
    "\n",
    "with h5py.File(input_filename, 'r') as input_file, h5py.File(output_filename, 'w') as output_file:\n",
    "    for token, token_group in input_file.items():\n",
    "        # Compute the mean of all datasets within the group\n",
    "        mean_embedding = np.mean([token_group[emb_id][...] for emb_id in token_group], axis=0)\n",
    "\n",
    "        # Create a new group in the output file with the same token name\n",
    "        output_token_group = output_file.create_group(token)\n",
    "\n",
    "        # Create a new dataset in the output group with the mean embedding\n",
    "        output_token_group.create_dataset('mean_embedding', data=mean_embedding)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKlearn (misery time!)\n",
    "\n",
    "We use k-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(n_neighbors=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(n_neighbors=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(n_neighbors=1)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "with h5py.File(output_filename, 'r') as input_file:\n",
    "    centroids = [input_file[token]['mean_embedding'][...] for token in input_file]\n",
    "    centroids = np.stack(centroids, axis=0)\n",
    "\n",
    "# Fit the NearestNeighbors model with the centroids\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='auto')\n",
    "nn.fit(centroids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "790f29072abc26870ccb3736e8ffe1b6fbe9bdb3e500c5faf362e772e52ef00f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
