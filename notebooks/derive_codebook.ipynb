{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 00:30:42 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": [
    "#%pip install fairseq\n",
    "import torch\n",
    "import fairseq\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.hub import download_url_to_file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook embeds a synthetic dataset of wav files <-> semantic token mappings and rederives the \"means\" for each token, allowing for later routine k-means inference of \"ground truth\" audio into semantic prompts. This allows for voice cloning and so giving voice to the dead, destroying consensus reality and creating misinformation, destroying copyright, spreading systemic bias, and other similarly enjoyable ways to spend a Saturday afternoon."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load HuBERT-formatted dataset\n",
    "\n",
    "NOTE: This guide presumes you auto-generated a synthetic dataset of examples using the `create_dataset` notebook earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 02:34:24 | INFO | fairseq.data.audio.hubert_dataset | max_keep=None, min_keep=None, loaded 16, skipped 0 short and 0 long, longest-loaded=160000, shortest-loaded=26454\n",
      "2023-04-26 02:34:24 | INFO | fairseq.data.audio.hubert_dataset | pad_audio=False, random_crop=False, normalize=False, max_sample_size=9223372036854775807\n"
     ]
    }
   ],
   "source": [
    "from fairseq.data import HubertDataset\n",
    "from bark.generation import SEMANTIC_RATE_HZ, SEMANTIC_PAD_TOKEN\n",
    "\n",
    "dataset_dir = os.path.join(\"../datasets/\", \"en\")\n",
    "manifest = f\"{dataset_dir}/manifest.tsv\"\n",
    "pad_list = [SEMANTIC_PAD_TOKEN]\n",
    "# I'm going to regret this\n",
    "eos_list = [SEMANTIC_PAD_TOKEN]\n",
    "paths = [f\"{dataset_dir}/labels.txt\"]\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "dataset = HubertDataset(\n",
    "    manifest,\n",
    "    sample_rate=16_000,\n",
    "    label_paths=paths,\n",
    "    label_rates=SEMANTIC_RATE_HZ,\n",
    "    pad_list=pad_list,\n",
    "    eos_list=eos_list,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../models/hubert_base_ls960.pt\"):\n",
    "    # Yes, hard-coding the URL of the model is jank. Too bad!\n",
    "    # Update this if this changes! https://github.com/facebookresearch/textlesslib/blob/698e6a039375bac0cd5f1b8683beeec5e8f702c0/textless/checkpoint_manager/__init__.py#L20\n",
    "    download_url_to_file(\"https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt\", \"../models/hubert_base_ls960.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy-pasted from textlesslib\n",
    "class HubertFeatureReader(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, checkpoint_path, layer=6, max_chunk=100 * 16_000, lazy_load=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # NB: fairseq doesn't support pathlib.Path\n",
    "        self.checkpoint_path = str(checkpoint_path)\n",
    "        self.should_normalize = False\n",
    "        self.lazy_load = lazy_load\n",
    "        self.model = None\n",
    "        self.layer = layer\n",
    "        self.max_chunk = max_chunk\n",
    "        # this is useful for determining the device\n",
    "        self.register_buffer(\"_float_tensor\", torch.tensor([0], dtype=torch.float))\n",
    "        if not self.lazy_load:\n",
    "            self.load_checkpoint_()\n",
    "\n",
    "    @torch.no_grad()  # otherwise some non-leaf nodes appear which breaks serialization\n",
    "    def load_checkpoint_(self):\n",
    "        model, _, task = fairseq.checkpoint_utils.load_model_ensemble_and_task(\n",
    "            [self.checkpoint_path]\n",
    "        )\n",
    "        self.model = model[0].eval()\n",
    "        self.model = self.model.to(self.device)\n",
    "        for parameter in self.model.parameters():\n",
    "            parameter.requires_grad_(False)\n",
    "\n",
    "        self.should_normalize = task.cfg.normalize\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self._float_tensor.device\n",
    "\n",
    "    @property\n",
    "    def code_hop_size(self) -> int:\n",
    "        return 320\n",
    "\n",
    "    @property\n",
    "    def expected_sample_rate(self) -> int:\n",
    "        return 16_000\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.lazy_load and self.model is None:\n",
    "            self.load_checkpoint_()\n",
    "\n",
    "        return self.get_features(x)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def get_features(self, x):\n",
    "        x = x.to(self.device)\n",
    "        if self.should_normalize:\n",
    "            x = F.layer_norm(x, x.shape)\n",
    "        x = x.view(1, -1)\n",
    "\n",
    "        feat = []\n",
    "        for start in range(0, x.size(1), self.max_chunk):\n",
    "            x_chunk = x[:, start : start + self.max_chunk]\n",
    "            feat_chunk, _ = self.model.extract_features(\n",
    "                source=x_chunk,\n",
    "                padding_mask=None,\n",
    "                mask=False,\n",
    "                output_layer=self.layer,\n",
    "            )\n",
    "            feat.append(feat_chunk)\n",
    "        return torch.cat(feat, 1).squeeze(0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ritsuko/projects/ai/audio/bark/datasets/en\n"
     ]
    }
   ],
   "source": [
    "print(dataset.audio_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, BatchSampler, RandomSampler\n",
    "\n",
    "class CustomBatchSampler(BatchSampler):\n",
    "    def __init__(self, sampler, batch_size, collater, drop_last):\n",
    "        super().__init__(sampler, batch_size, drop_last)\n",
    "        self.collater = collater\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        for idx in self.sampler:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                sources = [self.sampler.data_source[i] for i in batch]\n",
    "                # Ungodly hack, fix this later\n",
    "                sources = list(map(\n",
    "                    lambda s: {\n",
    "                        **s, \n",
    "                        \"label_list\": [torch.tensor(list(map(int, s[\"label_list\"][0].split(\" \"))))]\n",
    "                    }, \n",
    "                    sources\n",
    "                    ))\n",
    "                print(sources[0])\n",
    "                yield self.collater(sources)\n",
    "                batch = []\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield self.collater([self.sampler.data_source[i] for i in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 15, 'source': tensor([-3.6678e-03, -5.1473e-03, -4.3893e-03,  ...,  9.4807e-04,\n",
      "         4.8220e-05, -7.6926e-04]), 'label_list': [tensor([9863, 8840, 8840, 4262,  206,  206,  186, 2371, 2371,   10, 2371,  206,\n",
      "        2371,  186,  147,  239,   10, 9400, 3511, 2626, 2000,  560, 7551, 1369,\n",
      "         302,   10,  608, 1134, 7509, 6670, 9095,  366,   50,   10,    5, 1041,\n",
      "        6107, 1026, 1026, 4924,  648,  171,  429,  402,   41,   38,   38, 5505,\n",
      "        5760, 6245, 6245, 4738, 4738, 4683, 3221,  232,   10,   27, 3621,  181,\n",
      "         107,  417, 7352, 7439, 5337,  117,   17, 4608, 4077, 3376,   45,   10,\n",
      "          41,  429, 1463, 3840, 3890, 9623, 6428,  122,   59,   28,   28,   28,\n",
      "        5264, 8045, 1291, 3745, 2441,  166, 3363,   10,    5,    5, 2837, 2651,\n",
      "        4158, 6444,   17,   17, 4483, 2947, 9285,  555, 9285,  555, 8870, 2682,\n",
      "        2682,  259, 2682, 1487,   26,   26,   26,   26,  175,  736, 8665, 8844,\n",
      "        4213, 5128, 5750,  273, 7692, 5265,  117,   41,   10,   41, 9506, 6412,\n",
      "        9510, 9510,  956, 2320, 7283,  990,  575,   41,  934, 2190, 9209, 7433,\n",
      "        7858, 7858, 2689, 5459,  620,  207,   28,   28,   27,  107, 8045, 8538,\n",
      "        8538, 5557, 2829, 9017,   85,   85, 3378,  131,   17, 5081, 3527,  230,\n",
      "          56,  230, 9509,  783, 5128, 9987, 3002,  206,   10, 4262, 4262, 1710,\n",
      "        6592, 1710, 1184,   10, 5332, 6466, 2600, 9757, 2000, 9757,  601,  601,\n",
      "        7430,  206,  206,  186,   10,  186,  206,  351,  206,   10,  186,  186,\n",
      "          10,  186,  231, 3511])]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m custom_batch_sampler \u001b[39m=\u001b[39m CustomBatchSampler(sampler, batch_size, dataset\u001b[39m.\u001b[39mcollater, drop_last\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset, batch_sampler\u001b[39m=\u001b[39mcustom_batch_sampler)\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, items \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(items)\n\u001b[1;32m     10\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/ai/audio/bark/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/projects/ai/audio/bark/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/projects/ai/audio/bark/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/projects/ai/audio/bark/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/projects/ai/audio/bark/venv/lib/python3.10/site-packages/fairseq/data/audio/hubert_dataset.py:209\u001b[0m, in \u001b[0;36mHubertDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m--> 209\u001b[0m     wav \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_audio(index)\n\u001b[1;32m    210\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_labels(index)\n\u001b[1;32m    211\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: wav, \u001b[39m\"\u001b[39m\u001b[39mlabel_list\u001b[39m\u001b[39m\"\u001b[39m: labels}\n",
      "File \u001b[0;32m~/projects/ai/audio/bark/venv/lib/python3.10/site-packages/fairseq/data/audio/hubert_dataset.py:179\u001b[0m, in \u001b[0;36mHubertDataset.get_audio\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_audio\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m    177\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msoundfile\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msf\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     wav_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio_root, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maudio_names[index])\n\u001b[1;32m    180\u001b[0m     _path, slice_ptr \u001b[39m=\u001b[39m parse_path(wav_path)\n\u001b[1;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(slice_ptr) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "sampler = RandomSampler(dataset)\n",
    "custom_batch_sampler = CustomBatchSampler(sampler, batch_size, dataset.collater, drop_last=False)\n",
    "dataloader = DataLoader(dataset, batch_sampler=custom_batch_sampler)\n",
    "for batch_idx, items in enumerate(dataloader):\n",
    "    print(items)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "790f29072abc26870ccb3736e8ffe1b6fbe9bdb3e500c5faf362e772e52ef00f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
